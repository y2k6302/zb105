{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 函數定義-爬取網頁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getdetail(url , title, category, cur):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text)\n",
    "    dic = {\"summary\":soup.select('#summary')[0].text,\n",
    "           \"title\": title,\n",
    "           \"category\": category\n",
    "    }\n",
    "    sql = \"insert into news_main({}) values({})\"\n",
    "    sql2 = sql.format(','.join(dic.keys()), ','.join(len(dic) * '?'))\n",
    "    cur.execute(sql2, dic.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料存進 sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import sqlite3 \n",
    "con = sqlite3.connect('news.sqlite')\n",
    "cur = con.cursor()\n",
    "cur.execute(\"\"\"drop table news_main;\"\"\")\n",
    "cur.execute(\"\"\"create table news_main(summary text, title text, category varchar(20));\"\"\")\n",
    "con.commit()\n",
    "from bs4 import BeautifulSoup\n",
    "domain = 'http://www.appledaily.com.tw'\n",
    "for page in range(1,30):\n",
    "    res = requests.get('http://www.appledaily.com.tw/realtimenews/section/new/' + str(page))\n",
    "    soup = BeautifulSoup(res.text)\n",
    "    for rtddt in soup.select('.rtddt'):\n",
    "        title = rtddt.select('h1')[0].text\n",
    "        category = rtddt.select('h2')[0].text\n",
    "        if domain not in rtddt.select('a')[0]['href'] :\n",
    "            url = domain + rtddt.select('a')[0]['href'] \n",
    "        else:\n",
    "            url = rtddt.select('a')[0]['href'] \n",
    "        getdetail(url , title, category, cur)\n",
    "        con.commit()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jieba 斷詞demostrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "jieba.add_word('周子瑜')\n",
    "for ele in jieba.cut('韓國女團「TWICE」成員、16歲台灣正妹周子瑜，回台參加學力鑑定考試並與親友師長會面後，結束短短不到2天行程，今清晨5時35分在隨行人員陪同下抵達高雄小港機場，5時39分進入出境管制門，將搭乘上午7時華航CI0164班機直飛韓國仁川機場，預計上午10時45分抵達'):\n",
    "    print ele,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "ary = ['【更新】柯P：洪智坤洩漏公文案還沒看到公文　今處理',\n",
    "'留洪智坤 柯：殘障求職不易',\n",
    "'人事處議處洪智坤　柯P：不清楚議處結果']\n",
    "corpus = []\n",
    "for title in ary:\n",
    "    corpus.append('|'.join(jieba.cut(title)))\n",
    "print corpus[0]  \n",
    "print corpus[1]  \n",
    "print corpus[2]  \n",
    "vectorizer = CountVectorizer() \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "word = vectorizer.get_feature_names() \n",
    "for w in word:\n",
    "    print w,\n",
    "print \n",
    "print X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取資料庫，抓幾個類別進行測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3 \n",
    "import jieba\n",
    "con = sqlite3.connect('news.sqlite')\n",
    "cur = con.cursor()\n",
    "data = cur.execute(\"\"\"select * from news_main;\"\"\")\n",
    "corpus = []\n",
    "tags = []\n",
    "for rec in data:\n",
    "    if (rec[2] == '娛樂'.decode('utf-8')) or \\\n",
    "        (rec[2] == '社會'.decode('utf-8')) or \\\n",
    "        (rec[2] == '財經'.decode('utf-8')):\n",
    "        summary = rec[0]\n",
    "        corpus.append(' '.join(jieba.cut(summary)))\n",
    "        tags.append(rec[2]) \n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#使用 jieba 斷詞並結合 CountVectorizer 進行字詞的矩陣計算(有出現的自詞+1)\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer() \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "word = vectorizer.get_feature_names() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#進行資料訓練 (70%訓練 30%測試 )\n",
    "from sklearn.cross_validation import train_test_split\n",
    "train_data, test_data, train_tag, test_tag = train_test_split(X, tags, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#引進 naive_bayes 演算法，進行運算，看是否能正確分類『每篇新聞』屬於『哪一個類別』\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=0.01)   # naive_bayes 是用機率在計算，在這邊設定，即使沒有出現的字詞也給他 alpha=0.01 因為機率若為 0 會有問題\n",
    "clf.fit(train_data,train_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#進行預測\n",
    "pred = clf.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#計算演算法正確率\n",
    "from sklearn.metrics import accuracy_score\n",
    "print accuracy_score(test_tag, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#印出演算法分類結果\n",
    "from sklearn.metrics import confusion_matrix \n",
    "a = confusion_matrix(test_tag, pred) \n",
    "for ele in clf.classes_:\n",
    "    print ele,\n",
    "print\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#印出字詞出現次數，與分類相關資料\n",
    "index = 0\n",
    "coef_features_c1_c2_c3 = []\n",
    "import re\n",
    "for feat, c1, c2, c3 in zip(vectorizer.get_feature_names(), \\\n",
    "                        clf.feature_count_[0], clf.feature_count_[1], clf.feature_count_[2]):\n",
    "    coef_features_c1_c2_c3.append(tuple([clf.coef_[0][index], feat, c1, c2, c3]))\n",
    "    index+=1\n",
    "        \n",
    "for i in sorted(coef_features_c1_c2_c3):\n",
    "    if i[2] == max(i[2], i[3],i[4]) and i[2] >0 and  re.match(u\"[\\u4e00-\\u9fa5]+\",i[1]):\n",
    "        print \"娛樂\", i[1], i[2], i[3], i[4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
