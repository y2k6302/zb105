{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步驟一 載入資料庫&字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "jieba: file does not exist: E:\\text_mining\\dict.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-079bd5e61037>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'E:/text_mining/dict.txt'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#切換至中文繁體字庫\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_userdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'E:/text_mining/dict_keyw.txt'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_userdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'E:/text_mining/dict_cbdic.txt'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\BIG DATA\\Anaconda2\\lib\\site-packages\\jieba\\__init__.pyc\u001b[0m in \u001b[0;36mset_dictionary\u001b[1;34m(self, dictionary_path)\u001b[0m\n\u001b[0;32m    485\u001b[0m             \u001b[0mabs_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_abs_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionary_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"jieba: file does not exist: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mabs_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: jieba: file does not exist: E:\\text_mining\\dict.txt"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pymongo import MongoClient \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import re\n",
    "import jieba\n",
    "jieba.set_dictionary('E:/text_mining/dict.txt')  #切換至中文繁體字庫\n",
    "jieba.load_userdict('E:/text_mining/dict_keyw.txt')  #\n",
    "jieba.load_userdict('E:/text_mining/dict_cbdic.txt')  #\n",
    "#預設就是自己\n",
    "client = MongoClient('mongodb://10.120.28.14:27019')\n",
    "database = client['test']\n",
    "collection =database['test4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步驟二 切詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "tic = time.clock()\n",
    "\n",
    "title=[]\n",
    "content =[]\n",
    "#拿資料\n",
    "for post in collection.find(): \n",
    "    summary = post['content']\n",
    "    content.append(jieba.cut(summary))\n",
    "    title.append(post['title'])\n",
    "#總文章數量\n",
    "print \"文章數:\",len(title)\n",
    "toc = time.clock()\n",
    "print \"執行時間:\",(toc - tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步驟三 存放情緒詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#集合（Set）是無序、元素不重複的集合。(使用於字典檔時可以避免重複 ， 如果存jieba則不建議使用，尤其是計算tf-idf時)\n",
    "negative_set=set()  \n",
    "positive_set=set()\n",
    "\n",
    "f=open('E:/text_mining/negative.txt','r')\n",
    "for word in f.readlines():\n",
    "    word=word.strip()\n",
    "    if word not in negative_set:\n",
    "        negative_set.add(word)\n",
    "f.close()\n",
    "\n",
    "f=open('E:/text_mining/positive.txt','r')\n",
    "for word in f.readlines():\n",
    "    word=word.strip()\n",
    "    if word not in positive_set:\n",
    "        positive_set.add(word)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步驟四 取出各篇文章切詞 尋找正負詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tStart=time.time() #計算所需時間(計時開始)\n",
    "\n",
    "neg_write=[]\n",
    "pos_write=[]\n",
    "\n",
    "for n in range(0,6287):  #迴圈參考上面的總文章數\n",
    "\n",
    "    for word in content[n]:\n",
    "        #print word\n",
    "        if word.encode('utf-8') in negative_set:\n",
    "            neg_word_count=neg_word_count+1 # 為負面詞+1 計數\n",
    "        if word.encode('utf-8') in positive_set:\n",
    "            pos_word_count=pos_word_count+1 # 為正面詞+1 計數\n",
    "    neg_write.append(neg_word_count)  #單篇負面詞總數 存入陣列\n",
    "    pos_write.append(pos_word_count)  #單篇正面詞總數 存入陣列\n",
    "\n",
    "\n",
    "\n",
    "tEnd = time.time() #記時(時間結束)\n",
    "  \n",
    "print ' 花了 {} s'.format(tEnd-tStart)    #將經過時間print 出來"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  步驟五 存入資料庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "tStart=time.time() #計算所需時間(計時開始)\n",
    "\n",
    "obj = collection.find()  #這行一定要加\n",
    "\n",
    "for i in range(0,6287):  #迴圈參考上面的總文章數\n",
    "    collection.update({\"title\":obj[i][\"title\"]},{\"$set\":{\"pos\":pos_write[i],\"neg\":neg_write[i]}}) #collection.update 加入資料庫\n",
    "\n",
    "tEnd = time.time() #記時(時間結束)\n",
    "  \n",
    "print ' 花了 {} s'.format(tEnd-tStart)    #將經過時間print 出來"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步驟一 載入資料庫&字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from E:\\dics\\dict.txt.big.txt ...\n",
      "Loading model from cache c:\\users\\bigdat~1\\appdata\\local\\temp\\jieba.u56ec63d90e89b3354142c30b3eb89a90.cache\n",
      "Loading model cost 0.472 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pymongo import MongoClient \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import re\n",
    "import jieba\n",
    "jieba.set_dictionary('E:/dics/dict.txt.big.txt')  #切換至中文繁體字庫\n",
    "jieba.load_userdict('E:/dics/dict_keyw.txt')  #\n",
    "jieba.load_userdict('E:/dics/dict_cbdic.txt')  #\n",
    "#預設就是自己\n",
    "client = MongoClient('10.120.28.12',27017)\n",
    "database = client['test']\n",
    "collection =database['test2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步驟二 切詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "執行時間: 1.18134244909\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tic = time.clock()\n",
    "\n",
    "title=[]\n",
    "content =[]\n",
    "#拿資料\n",
    "for post in collection.find(): \n",
    "    summary = post['content']\n",
    "    content.append(jieba.cut(summary))\n",
    "#總文章數量\n",
    "toc = time.clock()\n",
    "print \"執行時間:\",(toc - tic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步驟三 存放情緒詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#集合（Set）是無序、元素不重複的集合。(使用於字典檔時可以避免重複 ， 如果存jieba則不建議使用，尤其是計算tf-idf時)\n",
    "negative_set=set()  \n",
    "positive_set=set()\n",
    "\n",
    "f=open('E:/dics/negative.txt','r')\n",
    "for word in f.readlines():\n",
    "    word=word.strip()\n",
    "    if word not in negative_set:\n",
    "        negative_set.add(word)\n",
    "f.close()\n",
    "\n",
    "f=open('E:/dics/positive.txt','r')\n",
    "for word in f.readlines():\n",
    "    word=word.strip()\n",
    "    if word not in positive_set:\n",
    "        positive_set.add(word)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步驟四 取出各篇文章切詞 尋找正負詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 花了 84.5399999619 s\n"
     ]
    }
   ],
   "source": [
    "tStart=time.time() #計算所需時間(計時開始)\n",
    "\n",
    "\n",
    "neg_write=[]\n",
    "pos_write=[]\n",
    "\n",
    "\n",
    "for n in range(0,41920):  #迴圈參考上面的總文章數\n",
    "    neg=[]\n",
    "    pos=[]\n",
    "    for word in content[n]:\n",
    "        \n",
    "        #print word\n",
    "        if word.encode('utf-8') in negative_set:\n",
    "            neg.append(word.encode('utf-8'))\n",
    "        if word.encode('utf-8') in positive_set:\n",
    "            pos.append(word.encode('utf-8'))\n",
    "    \n",
    "    neg_write.append(neg)  #單篇負面詞總數 存入陣列\n",
    "    pos_write.append(pos)  #單篇正面詞總數 存入陣列\n",
    "\n",
    "\n",
    "\n",
    "tEnd = time.time() #記時(時間結束)\n",
    "  \n",
    "print ' 花了 {} s'.format(tEnd-tStart)    #將經過時間print 出來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print len(pos_write[41919])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  步驟五 存入資料庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 花了 261.467999935 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tStart=time.time() #計算所需時間(計時開始)\n",
    "\n",
    "obj = collection.find()  #這行一定要加\n",
    "\n",
    "for i in range(0,41920):  #迴圈參考上面的總文章數\n",
    "    collection.update({\"_id\":obj[i][\"_id\"]},{\"$set\":{\"pos\":pos_write[i],\"neg\":neg_write[i]}}) #collection.update 加入資料庫\n",
    "\n",
    "tEnd = time.time() #記時(時間結束)\n",
    "  \n",
    "print ' 花了 {} s'.format(tEnd-tStart)    #將經過時間print 出來"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 計算情緒詞數量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "{u'\\u62ac\\u982d': 1, u'\\u6d6a\\u6f2b': 1, u'\\u65b0\\u589e': 1, u'\\u9084\\u8981': 1, u'\\u6e67\\u5165': 1, u'\\u4eab\\u53d7': 1, u'\\u7279\\u5730': 1, u'\\u958b\\u51fa': 1, u'\\u90fd\\u662f': 1, u'\\u6f02\\u4eae': 1, u'\\u71e6\\u721b': 1, u'\\u7dca\\u76ef': 1, u'\\u7dca\\u63e1': 1, u'\\u7f8e\\u9e97': 2, u'\\u540c\\u6b65': 1, u'\\u6f14\\u51fa': 1, u'\\u6b23\\u8cde': 1, u'\\u8fce\\u63a5': 1, u'\\u60f3\\u50cf': 1, u'\\u5f88\\u6f02\\u4eae': 1, u'\\u97f3\\u6a02': 1, u'\\u865f\\u7a31': 1, u'\\u5e0c\\u671b': 1, u'\\u8208\\u596e': 1, u'\\u6700\\u9577': 2, u'\\u71b1\\u9b27': 1, u'\\u7480\\u74a8': 1}\n",
      "------------------------------------------------------------------\n",
      "{u'\\u6f14\\u51fa': 3, u'\\u9996\\u5ea6': 1, u'\\u6027\\u611f': 2, u'\\u71b1\\u529b': 2, u'\\u4e3b\\u6301': 1, u'\\u76f4\\u5230': 1, u'\\u4eba\\u6c23': 2, u'\\u66f4\\u662f': 1, u'\\u6e67\\u5165': 3, u'\\u5929\\u540e': 2, u'\\u8d95\\u5230': 1, u'\\u5927\\u958b': 1, u'\\u5171\\u540c': 1, u'\\u8fce\\u63a5': 1}\n",
      "------------------------------------------------------------------\n",
      "{u'\\u5e36\\u7d66': 1, u'\\u7480\\u74a8': 1, u'\\u5149\\u8292': 1, u'\\u5de7\\u5999': 1, u'\\u6700\\u9577': 1, u'\\u8fce\\u5411': 1, u'\\u81ea\\u7136': 1}\n"
     ]
    }
   ],
   "source": [
    "#計算個別文章的情緒詞數量\n",
    "for post in collection.find().limit(3):\n",
    "    s = post['pos']\n",
    "    #print s\n",
    "    print \"------------------------------------------------------------------\"\n",
    "    dic = {}\n",
    "    for ele in s: # n\n",
    "        if not ele in dic:\n",
    "            dic[ele] = 1\n",
    "        else:\n",
    "            dic[ele] = dic[ele] + 1\n",
    "    print dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'\\u62ac\\u982d': 1, u'\\u81ea\\u7136': 1, u'\\u6d6a\\u6f2b': 1, u'\\u65b0\\u589e': 1, u'\\u5de7\\u5999': 1, u'\\u9084\\u8981': 1, u'\\u5149\\u8292': 1, u'\\u6e67\\u5165': 4, u'\\u4eab\\u53d7': 1, u'\\u76f4\\u5230': 1, u'\\u7279\\u5730': 1, u'\\u958b\\u51fa': 1, u'\\u90fd\\u662f': 1, u'\\u5171\\u540c': 1, u'\\u6027\\u611f': 2, u'\\u6f02\\u4eae': 1, u'\\u71e6\\u721b': 1, u'\\u7dca\\u76ef': 1, u'\\u5929\\u540e': 2, u'\\u5927\\u958b': 1, u'\\u7dca\\u63e1': 1, u'\\u7f8e\\u9e97': 2, u'\\u540c\\u6b65': 1, u'\\u6f14\\u51fa': 4, u'\\u5e36\\u7d66': 1, u'\\u6b23\\u8cde': 1, u'\\u71b1\\u529b': 2, u'\\u8fce\\u63a5': 2, u'\\u8d95\\u5230': 1, u'\\u60f3\\u50cf': 1, u'\\u4eba\\u6c23': 2, u'\\u5f88\\u6f02\\u4eae': 1, u'\\u9996\\u5ea6': 1, u'\\u97f3\\u6a02': 1, u'\\u865f\\u7a31': 1, u'\\u5e0c\\u671b': 1, u'\\u66f4\\u662f': 1, u'\\u4e3b\\u6301': 1, u'\\u8208\\u596e': 1, u'\\u6700\\u9577': 3, u'\\u71b1\\u9b27': 1, u'\\u8fce\\u5411': 1, u'\\u7480\\u74a8': 2}\n"
     ]
    }
   ],
   "source": [
    "#計算所有目標新聞的情緒詞數量\n",
    "import json\n",
    "all_pos=[]\n",
    "for post in collection.find().limit(3):\n",
    "    s = post['pos']\n",
    "    for i in s:\n",
    "        all_pos.append(i)\n",
    "\n",
    "dic = {}\n",
    "for ele in all_pos: # n\n",
    "    if not ele in dic:\n",
    "        dic[ele] = 1\n",
    "    else:\n",
    "        dic[ele] = dic[ele] + 1\n",
    "print dic\n",
    "    #all_pos_count.append(dic)\n",
    "#sentimental_json =json.dumps(dic)\n",
    "#print sentimental_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'count': 1, 'pos': u'\\u62ac\\u982d'}, {'count': 1, 'pos': u'\\u6d6a\\u6f2b'}, {'count': 1, 'pos': u'\\u65b0\\u589e'}, {'count': 1, 'pos': u'\\u9084\\u8981'}, {'count': 1, 'pos': u'\\u4eab\\u53d7'}, {'count': 1, 'pos': u'\\u76f4\\u5230'}, {'count': 1, 'pos': u'\\u7279\\u5730'}, {'count': 1, 'pos': u'\\u958b\\u51fa'}, {'count': 1, 'pos': u'\\u90fd\\u662f'}, {'count': 1, 'pos': u'\\u5171\\u540c'}, {'count': 1, 'pos': u'\\u6f02\\u4eae'}, {'count': 1, 'pos': u'\\u71e6\\u721b'}, {'count': 1, 'pos': u'\\u7dca\\u76ef'}, {'count': 1, 'pos': u'\\u5927\\u958b'}, {'count': 1, 'pos': u'\\u7dca\\u63e1'}, {'count': 1, 'pos': u'\\u540c\\u6b65'}, {'count': 1, 'pos': u'\\u6b23\\u8cde'}, {'count': 1, 'pos': u'\\u8d95\\u5230'}, {'count': 1, 'pos': u'\\u60f3\\u50cf'}, {'count': 1, 'pos': u'\\u5f88\\u6f02\\u4eae'}, {'count': 1, 'pos': u'\\u9996\\u5ea6'}, {'count': 1, 'pos': u'\\u97f3\\u6a02'}, {'count': 1, 'pos': u'\\u865f\\u7a31'}, {'count': 1, 'pos': u'\\u5e0c\\u671b'}, {'count': 1, 'pos': u'\\u66f4\\u662f'}, {'count': 1, 'pos': u'\\u4e3b\\u6301'}, {'count': 1, 'pos': u'\\u8208\\u596e'}, {'count': 1, 'pos': u'\\u71b1\\u9b27'}, {'count': 1, 'pos': u'\\u7480\\u74a8'}, {'count': 2, 'pos': u'\\u6027\\u611f'}, {'count': 2, 'pos': u'\\u5929\\u540e'}, {'count': 2, 'pos': u'\\u7f8e\\u9e97'}, {'count': 2, 'pos': u'\\u71b1\\u529b'}, {'count': 2, 'pos': u'\\u8fce\\u63a5'}, {'count': 2, 'pos': u'\\u4eba\\u6c23'}, {'count': 2, 'pos': u'\\u6700\\u9577'}, {'count': 4, 'pos': u'\\u6e67\\u5165'}, {'count': 4, 'pos': u'\\u6f14\\u51fa'}]\n",
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "#單算正向詞\n",
    "import json\n",
    "all_pos_word=[]\n",
    "all_pos=[]  #所有正向詞\n",
    "for post in collection.find().limit(2):\n",
    "    pos = post['pos']\n",
    "    for i in pos:\n",
    "        all_pos.append(i)\n",
    "pos_dic = {}\n",
    "for ele in all_pos: # n\n",
    "    if not ele in pos_dic:\n",
    "        pos_dic[ele] = 1\n",
    "    else:\n",
    "        pos_dic[ele] = pos_dic[ele] + 1\n",
    "\n",
    "       \n",
    "for i in range(0,len(pos_dic)):\n",
    "    data = {\n",
    "        \"pos\":pos_dic.keys()[i],\n",
    "        \"count\":pos_dic.values()[i]\n",
    "    }\n",
    "    all_pos_word.append(data)\n",
    "    \n",
    "print all_pos_count\n",
    "print type(all_pos_count)\n",
    "#pos_sentimental_json =json.dumps(all_pos_word)\n",
    "#print pos_sentimental_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[{'count': 4, 'pos': u'\\u6e67\\u5165'}, {'count': 4, 'pos': u'\\u6f14\\u51fa'}, {'count': 2, 'pos': u'\\u6027\\u611f'}, {'count': 2, 'pos': u'\\u5929\\u540e'}, {'count': 2, 'pos': u'\\u7f8e\\u9e97'}, {'count': 2, 'pos': u'\\u71b1\\u529b'}, {'count': 2, 'pos': u'\\u8fce\\u63a5'}, {'count': 2, 'pos': u'\\u4eba\\u6c23'}, {'count': 2, 'pos': u'\\u6700\\u9577'}, {'count': 1, 'pos': u'\\u62ac\\u982d'}, {'count': 1, 'pos': u'\\u6d6a\\u6f2b'}, {'count': 1, 'pos': u'\\u65b0\\u589e'}, {'count': 1, 'pos': u'\\u9084\\u8981'}, {'count': 1, 'pos': u'\\u4eab\\u53d7'}, {'count': 1, 'pos': u'\\u76f4\\u5230'}, {'count': 1, 'pos': u'\\u7279\\u5730'}, {'count': 1, 'pos': u'\\u958b\\u51fa'}, {'count': 1, 'pos': u'\\u90fd\\u662f'}, {'count': 1, 'pos': u'\\u5171\\u540c'}, {'count': 1, 'pos': u'\\u6f02\\u4eae'}, {'count': 1, 'pos': u'\\u71e6\\u721b'}, {'count': 1, 'pos': u'\\u7dca\\u76ef'}, {'count': 1, 'pos': u'\\u5927\\u958b'}, {'count': 1, 'pos': u'\\u7dca\\u63e1'}, {'count': 1, 'pos': u'\\u540c\\u6b65'}, {'count': 1, 'pos': u'\\u6b23\\u8cde'}, {'count': 1, 'pos': u'\\u8d95\\u5230'}, {'count': 1, 'pos': u'\\u60f3\\u50cf'}, {'count': 1, 'pos': u'\\u5f88\\u6f02\\u4eae'}, {'count': 1, 'pos': u'\\u9996\\u5ea6'}, {'count': 1, 'pos': u'\\u97f3\\u6a02'}, {'count': 1, 'pos': u'\\u865f\\u7a31'}, {'count': 1, 'pos': u'\\u5e0c\\u671b'}, {'count': 1, 'pos': u'\\u66f4\\u662f'}, {'count': 1, 'pos': u'\\u4e3b\\u6301'}, {'count': 1, 'pos': u'\\u8208\\u596e'}, {'count': 1, 'pos': u'\\u71b1\\u9b27'}, {'count': 1, 'pos': u'\\u7480\\u74a8'}]\n"
     ]
    }
   ],
   "source": [
    "#進行排序\n",
    "print all_pos_count.sort(key=lambda d:d['count'],reverse=True)\n",
    "print all_pos_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
